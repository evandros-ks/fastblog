{
  
    
        "post0": {
            "title": "Medical Imaging Preprocessing",
            "content": "Background . In this tutorial, we will review how to process and handle DICOM data. A great resource to access medical imaging data is through the Cancer Imaging Archive. They maintain an archive of medical images across various datasets that are accessible for public download. We will be using scans from the LIDC-IDRI dataset which contains diagnostic and lung cancer screening thoracic computed tomography (CT) scans with annotated lesions in XML format. To handle the annotated lessions, researchers have developed the pylidc for processing the XML files. . PATH = &#39;data/LIDC-IDRI&#39; def get_dicom_directories(input_dir: str) -&gt; list: &#39;&#39;&#39;&#39;Get directory up until parent of all subdirectories that contain DICOM files Args: input_dir (str): path to the input directory of DICOM files Returns: list: subdirectories in input_dir that contain DICOM files Raises: ValueError: checks if any DICOM files exist in input_dir &#39;&#39;&#39; dicom_directories = list(set([path.parent for path in Path(input_dir).rglob(&#39;*.dcm&#39;)])) if not dicom_directories: raise ValueError(f&#39;{input_dir} contains no DICOM files&#39;) return dicom_directories . get_dicom_directories(PATH)[:3] . [PosixPath(&#39;data/LIDC-IDRI/LIDC-IDRI-0003/01-01-2000-NA-NA-59141/3000978.000000-NA-59882&#39;), PosixPath(&#39;data/LIDC-IDRI/LIDC-IDRI-0008/01-01-2000-NA-NA-30141/3000549.000000-NA-21954&#39;), PosixPath(&#39;data/LIDC-IDRI/LIDC-IDRI-0004/01-01-2000-NA-NA-91780/3000534.000000-NA-58228&#39;)] . DICOM files . Without going into intensive detail, we should know that CT scans produce an image by producing cross-sectional images from an X-Ray and then reconstruct the slices to form a volumetric image. Each slice is saved to a DICOM file which stores the pixel array and metadata for that particular slice. We can sort the list of DICOM files by the image position of the patient to construct a volumetric image and then sort the slices in ascending order. If you&#39;re interested in learning about the physics of medical imaging and how images are produced, I recommend Fundamentals of Medical Imaging. . After sorting DICOM files by SliceLocation, we can load the DICOM files in the correct order. Looking at the first DICOM file, we can see all the metadata information that is stored. We&#39;ll need this to process and store our data in a more appropriate format. . DICOM or Digital Imaging and Communications in Medicine is a standardized file format for medical imaging information and related data. Typically, you will see the raw medical images stored in this format and later processed to an easier to handle format such as NIfTI or Nrrd. To process our DICOM files, we are using the pydicom library and have constructed and read a list of pydicom.dataset.FileDataset objects with our scan list. This object allows to access any relevant data elements stored in the DICOM file, useful data elements include Modality, RescaleIntercept, RescaleSlope, PixelSpacing, SliceThickness, and ImagePositionPatient. We&#39;ll need these data elements to standardize our scans to the same pixel spacing and covert our pixel values to the Hounsfield scale. It could also be helpful to use these data elements and construct an array of metadata elements to use for training. This is particularly useful in case you want to identify any subtleties between CT manufacturers. The DICOM library has an exhaustive list of all the data elements and there use. You will also notice the following standardized format for DICOM data elements: . a tag that identifies the attribute, usually in the format (XXXX,XXXX) with hexadecimal numbers, and may be divided further into DICOM Group Number and DICOM Element Number; | a DICOM Value Representation (VR) that describes the data type and format of the attribute value. | . class DicomInterface: &#39;&#39;&#39;Class to help manipulate DICOM files in a directory Args: input_dir (str): directory containing all dcm files &#39;&#39;&#39; def __init__(self, input_dir: str): self.input_dir = input_dir self._load_dicom() def _load_dicom(self): self._read_dicom() if self.modality == &#39;CT&#39;: # sort slices vertically self.dicoms.sort(key = lambda dcm: float(dcm.SliceLocation)) def _read_dicom(self): &#39;&#39;&#39;Loads all dcm files and sorts them according to SliceLocation if possible&#39;&#39;&#39; # read all .dcm files given parent directory dicoms = [pydicom.read_file(f&quot;{dcm}&quot;) for dcm in glob.glob(f&quot;{str(self.input_dir)}/*.dcm&quot;)] self.dicoms = dicoms @property def modality(self): &#39;&#39;&#39;Modality of DICOM files Tag - Name: (0008, 0060) - Modality &#39;&#39;&#39; _modality = self.dicoms[0].Modality return _modality . def dicom_filter(input_dir: str, config: dict) -&gt; list: &#39;&#39;&#39;Filter DICOM series from the input directory based on a configuration dict Args: input_dir (str): path to the input directory of DICOM files config (dict): a dict object specifying what DICOM tags &amp; values to use to filter the DICOM series example: {&#39;Modality&#39;: [&#39;CT&#39;]} Returns: list: matching DICOM series &#39;&#39;&#39; dicom_dirs = get_dicom_directories(input_dir) dcms = [DicomInterface(dcm) for dcm in dicom_dirs] filtered_dcms = [d for d in dcms for k,v in config.items() if getattr(d.dicoms[0], k) in v] return filtered_dcms . config = {&#39;Modality&#39;: [&#39;CT&#39;, &#39;DX&#39;]} . filtered_dcms = dicom_filter(PATH, config) print(len(filtered_dcms)) . 20 . Processing CT Scans . Hounsfield Scale . For CT scans, we are able to acquire an image by measuring how much the object attenuates the incidence X-Rays. The more dense the object, the greater it attenuates the incidence waves. From this object-wave interaction, we can expand this relationship to various tissue types and estimate tissues based on the energy received. The quantitative value describing this radiodensity is known as the Hounsfield scale also termed CT number when used for CT scans. A list of various tissue types and their HU value can be seen below. . The pixel information in the DICOM file does not store the exact HU number. However, it stores the slope and intercept value needed to scale these pixels to their respective HU number. This linear relationship is described below: . $ textrm{HU} = textrm{pixel_value} times textrm{slope} + textrm{intercept}$ . After processing our pixel array and converting it to Hounsfield scale, we can check the distribution of value. This bimodal distribution stems from the drastic differences in tissue properties in the acquire image. The air in the lungs and outside the body range from -1000 to -500 HU while more denser materials such as fat and cortical bone provide higher HU values of -100 to +3000 HU respectively. We&#39;ll need to remember this subtlety when normalizing our images later. . Resampling . We&#39;re nearly done processing our scans, but first let&#39;s resample the scans to the appropriate pixel spacing and slice thickness. We&#39;ll need to resample each scan to the same pixel spacing since regions of interest such as lesions or organs may be of completely different scale from patient to patient. . class ProcessData: &#39;&#39;&#39;Class to process and resample DICOM data Args: dcm (DicomInterface): Dicom object of data to be processed &#39;&#39;&#39; def __init__(self, dcm: DicomInterface): self.dcm = dcm def load_volume(self, new_spacing: np.ndarray = np.array([])) -&gt; np.ndarray: &#39;&#39;&#39;Load volume from dicom pixel array, convert to HU, and optionally resample Args: new_spacing (np.ndarray): New pixel spacing of dicom volume Returns: np.ndarray of HU pixel volume and optionally resampled &#39;&#39;&#39; volume = np.stack([s.pixel_array for s in self.dcm.dicoms]) processed_volume = self._convert_to_hu(volume) if new_spacing.any(): processed_volume = self._resample_volume(processed_volume, new_spacing) return processed_volume def _resample_volume(self, volume: np.ndarray, new_spacing: np.ndarray) -&gt; np.ndarray: &#39;&#39;&#39;Resample volume to new pixel spacing. Args: volume (np.ndarray): Volume of pixel array with original pixel spacing new_spacing (np.ndarray): New pixel spacing of volumed pixel array Returns: np.ndarray of resampled volume &#39;&#39;&#39; resize_factor = self.pixel_spacing / new_spacing resampled_volume = scipy.ndimage.interpolation.zoom(volume, resize_factor, mode=&#39;nearest&#39;) return resampled_volume def _convert_to_hu(self, volume: np.ndarray) -&gt; np.ndarray: &#39;&#39;&#39;Scale original dicom pixel array volume to HU. Args: volume (np.ndarray): Volume of unscaled pixel array Returns: np.ndarray rescaled to HU &#39;&#39;&#39; slope = self.dcm.dicoms[0].RescaleSlope intercept = self.dcm.dicoms[0].RescaleIntercept processed_volume = volume * slope + intercept processed_volume[processed_volume &lt; -1024] = -1024 return processed_volume @property def modality(self): &#39;&#39;&#39;Modality of DICOM files Tag - Name: (0008, 0060) - Modality &#39;&#39;&#39; _modality = self.dcm.dicoms[0].Modality return _modality @property def pixel_spacing(self): &#39;&#39;&#39;Pixel spacing of loaded volume in (z,y,x) format Tag - Name: (0018, 0050) - Slice Thickness Tag - Name: (0028, 0030) - Pixel Spacing &#39;&#39;&#39; slice_thickness = float(self.dcm.dicoms[0].SliceThickness) yx_spacing = tuple(np.array(self.dcm.dicoms[0].PixelSpacing, dtype=float)) _pixel_spacing = (slice_thickness,) + yx_spacing return np.array(_pixel_spacing) . dcm = filtered_dcms[1] dcm.modality . &#39;CT&#39; . %%time data = ProcessData(dcm) volume = data.load_volume(new_spacing=np.array([1,1,1])) # volume = data.load_volume() . CPU times: user 17.4 s, sys: 1.12 s, total: 18.5 s Wall time: 18.5 s . plt.hist(volume.ravel(), bins=50) print(f&quot;{volume.min()=}, {volume.max()=}&quot;) plt.show() . volume.min()=-1274.1161877472223, volume.max()=3757.5593412343073 . Since we&#39;re working with grayscale images, the intensity of the black varies according to the minimum and maximum values. The black pixels in the HU images are equivalent (-1024 HU) to the pixels in the lung and outside the body for the raw DICOM pixel array. Since we masked all pixels &lt; -1024, we no longer obtain a cylinder surrounding the body and set them all equivalent to air. If you&#39;re curious why we&#39;re obtaining a circle around each slice, it&#39;s because of the CT scan itself and how it emits and receives X-Rays all around the patient&#39;s body. However, since we resample our volume, there&#39;s slight error in the HU pixel valu . f, axs = plt.subplots(1,2, figsize=(12,12)) m1 = volume.shape[0]//2 m2 = len(dcm.dicoms)//2 axs[0].imshow(volume[m1,...], cmap=plt.cm.gray) axs[0].set_title(&#39;Processed to HU&#39;) axs[1].imshow(dcm.dicoms[m2].pixel_array, cmap=plt.cm.gray) axs[1].set_title(&#39;Raw DICOM pixel array&#39;) for a in axs: a.axis(&#39;off&#39;) f.show() . Image Windowing . Image windowing is extensively used by radiologists to view anatomical structures. It enhances the contrast for grayscale images by manipulating CT numbers to particular ranges. The brightness of the image is able to be adjusted via the window level/center and the contrast of the image via the window width. Typically, medical images viewers such as Slicer allow the user to adjust these values relatively easily. However, we will show a simple example here how windowing works. It&#39;s important to note that window centers and widths can also vary depending on the manufacturer. Generally, the following window levels and widths listed in HU are used to view particular anatomical regions: . head and neck brain W:80 L:40 | subdural W:130-300 L:50-100 | stroke W:8 L:32 or W:40 L:40 | temporal bones W:2800 L:600 or W:4000 L:700 | soft tissues: W:350–400 L:20–60 | . | chest lungs W:1500 L:-600 | mediastinum W:350 L:50 | . | abdomen soft tissues W:400 L:50 | liver W:150 L:30 | . | spine soft tissues W:250 L:50 | bone W:1800 L:400 | . | . We can either grab the window center and width from our DICOM metadata or check from the values above. To access it from the DICOM metadata, we would simply access the WindowCenter or WindowCenter property from our pydicom.dataset.FileDataset object. . window_dict = {&#39;head and neck&#39;: {&#39;brain&#39;: (40, 80), &#39;subdural&#39;: (75, 215), &#39;stroke&#39;: (32,8), &#39;temporal bones&#39;: (600, 2800), &#39;soft tissues&#39;: (40, 375)}, &#39;chest&#39;: {&#39;lungs&#39;: (-600,1500), &#39;mediastinum&#39;: (50, 350)}, &#39;abdomen&#39;: {&#39;soft tissue&#39;: (50, 400), &#39;liver&#39;: (30, 150)}, &#39;spine&#39;: {&#39;soft tissue&#39;: (50, 250), &#39;bone&#39;: (400, 1800)} } . def window_image(volume: np.ndarray, window_center: int, window_width: int): &#39;&#39;&#39;Windows the CT scan to a desired range of values &#39;&#39;&#39; img_min = window_center - window_width // 2 img_max = window_center + window_width // 2 window_volume = volume.copy() window_volume[window_volume &lt; img_min] = img_min window_volume[window_volume &gt; img_max] = img_max return window_volume . data.dcm.dicoms[0].WindowCenter, data.dcm.dicoms[0].WindowWidth . (&#39;-600.0&#39;, &#39;1600.0&#39;) . region = &#39;chest&#39; sub_region = &#39;lungs&#39; center, width = window_dict[region][sub_region] print(f&quot;{center=}&quot;) print(f&quot;{width=}&quot;) image = window_image(volume, center, width) m = volume.shape[0]//2 f, axs = plt.subplots(1,2,figsize=(10,10)) axs[0].imshow(volume[m,...],cmap=plt.cm.gray) axs[0].set_title(&#39;No window&#39;) axs[1].imshow(image[m,...],cmap=plt.cm.gray) axs[1].set_title(f&#39;Window center: {center} Window width: {width}&#39;) for a in axs: a.axis(&#39;off&#39;) f.show() . center=-600 width=1500 . SimpleITK . We showed how we were able to read and process manipulate with pydicom and the numpy library. However, SimpleITK has provided a wrapper on top of the Insight Segmentation and Registration Toolkit. This library simplifies the process to perform common tasks for image analysis, particularly with medical imaging data. . dirs = get_dicom_directories(PATH) . reader = sitk.ImageSeriesReader() dicom_names = reader.GetGDCMSeriesFileNames(str(dirs[1])) reader.SetFileNames(dicom_names) image = reader.Execute() image.GetSize(), image.GetSpacing() . ((512, 512, 133), (0.78125, 0.78125, 2.5000000075757574)) . array = sitk.GetArrayFromImage(image) . plt.hist(array.ravel(), bins=50) print(f&quot;{array.min()=}, {array.max()=}&quot;) plt.show() . array.min()=-2048, array.max()=3071 . m = array.shape[0]//2 plt.imshow(array[m,...]) . &lt;matplotlib.image.AxesImage at 0x7f498c0349a0&gt; . image.GetSize(), image.GetSpacing() . ((512, 512, 133), (0.78125, 0.78125, 2.5000000075757574)) . new_spacing = np.array([1,1,1]) new_size = np.rint(image.GetSize()*np.array(image.GetSpacing())/new_spacing).astype(int) new_spacing.tolist(), new_size.tolist() . ([1, 1, 1], [400, 400, 333]) . %%time original_size = image.GetSize() original_spacing = image.GetSpacing() new_spacing = np.array([1,1,1]) new_size = np.rint(image.GetSize()*np.array(image.GetSpacing())/new_spacing).astype(int) new_spacing, new_size = new_spacing.tolist(), new_size.tolist() resampled = sitk.Resample(image1=image, size=new_size, transform=sitk.Transform(), interpolator=sitk.sitkLinear, outputOrigin=image.GetOrigin(), outputSpacing=new_spacing, outputDirection=image.GetDirection(), defaultPixelValue=0, outputPixelType=image.GetPixelID()) resampled_array = sitk.GetArrayFromImage(resampled) . CPU times: user 1.64 s, sys: 359 ms, total: 2 s Wall time: 510 ms . Notice the speedup in resampling with SimpleITK rather than numpy. This process is significantly faster! . m2 = resampled_array.shape[0]//2 plt.imshow(resampled_array[m2,...]) resampled_array.shape . (333, 400, 400) .",
            "url": "https://evandros-ks.github.io/fastblog/2022/04/21/Medical_Image_Preprocessing.html",
            "relUrl": "/2022/04/21/Medical_Image_Preprocessing.html",
            "date": " • Apr 21, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Tabu Search",
            "content": "Tabu Search . Tabu Search is a meta-heuristic optimization algorithm leveraging local search strategies and memory structures. Since local search has the tendency to get stuck in local optima, it uses memory structures to help escape these optima by using an explorative strategy and avoiding previously visited nodes. The idea of prohibiting previously visited nodes is where the name tabu comes from. . Local Search . The first main concept we&#39;ll go over is the idea of Local Search. In Local Search, we start with an initial viable solution and generate a set of neighbouring solutions. We then iterate through each solution of the set and continue the next iteration with the best neighbouring solution. We continue this algorithm until our termination criteria is met. . def generate_first_solution(*args): pass def generate_neighbourhood(*args): pass def objective_function(*args): pass def check_termination_criteria(*args): pass current_solution = generate_first_solution() termination_criteria = False while not termination_criteria: # initialize best neighbouring objective value and generate neighbours best_n_objective_val = objective_function(current_solution) neighbours = generate_neighbourhood(current_solution) # iterate each neighbour for n in neighbours: current_objective_val = objective_function(n) # find the best neighbour objective value to use for the next iteration # we&#39;re assuming we want to minimize the objective value in this scenario if current_objective_val &lt; best_n_objective_val: best_n_objective_val = current_objective_val current_solution = n # check if termination criteria has been met yet if check_termination_criteria(): termination_criteria = True . Although this approach is quite straightforward and quick to implement, it comes with its pitfalls. In some cases, it becomes computationally expensive to search through all the possible neighbours of the current solution. On the other hand, considering only the immediate neighbours yields a very limited horizon and is also not efficient. This makes our algorithm susceptible to being stuck in local optima based on our neighbourhood generation. The difference between local and global optima convergence can be seen in the example below. . Memory . In order to avoid the pitfall of local optima, Tabu Search incorporates two different memory structures. Short-term memory is based on the recency of occurrence to avoid previously visited solutions. This short-term structure could also be used to tabulate viable solutions and intensify the search at these solutions. Long-term memory is based on the frequency of occurrence starting from the beginning of the optimization. By keeping track of frequently visited solutions, it can diversify the search space by avoiding frequently visited solutions. . The short-term memory component is formerly known as the Tabu List or intensification. This structure stores a fixed and limited number of solutions for a number of iterations, $T$, also known as the Tabu Tenure. . The long-term memory component stores the frequency of occurrence and is also referred to as diversification. Although Tabu Lists are useful to locally optimize the best known solution, it may be too local in its approach and miss good unexplored solutions. Diversification overcomes this problem through two main approaches: . Restart Diversification: Restart search space at unexplored solutions | Continuous Diversification: Penalizing the objective function through a frequency-memory term of the current solution | . Tabu Tenure . As previously mentioned, the Tabu Tenure defines the size of the Tabu List. This proves to be an important parameter for Tabu Search since it&#39;s responsible for helping the search escape local optima. If these search spaces are bigger than the Tabu Tenure our method would not be able to escape these cycles. Two main methods are proposed for defining this parameter: . Static: Select $T$ as a constant value ($ sqrt{n}$ problem size) | Dynamic: Random: Select $T$ to vary randomly between $T_{min}$ and $T_{max}$ | Systemic: Select a sequence of tenure values $T$ that is repeated throughout the search | Time-dependent: Tabu tenure progressively decreases according to time or number of iterations in order to gradually reduce diversification level | . | . Aspiration Criteria . An optional component of Tabu Search is the use of an Aspiration Criteria. This allows a move to be made even if it&#39;s in the Tabu List which helps prevent stagnation. Common examples of Aspiration Criteria include: . if the move yields a better solution than any solution obtained so far | if the move yields a better solution than an aspiration value (defined depending on optimization problem) | if the direction of the search (improving or non-improving) does not change | . Termination Criteria . After completing an iteration cycle, the algorithm must check if any termination criteria has been met yet. These stopping conditions include: . Evidence shows that the optimal solution has been found | No feasible solutions in current neighbourhood generation | Completed the maximum number of iterations | An improved solution hasn&#39;t been found in a specified number of iterations | . Algorithm . Choose an initial solution $s$ in $X$ for the first iteration $k=1$ | Generate neighbouring solutions $V^{*} subseteq N(s,k) $ for the current solution $s$ and iteration $k$ | Select the best solution $s&#39;$ in our current neighbourhood $V^{*}$ that&#39;s not tabu $T(s)$ or that meets our aspiration criteria $A(s)$ | Update Tabu List $T(s)$ by removing each solution past the Tabu Tenure, incrementing each counter, and adding our current solution $s&#39;$ | If the termination criteria are met stop the search else continue from step 2 with our best neighbourhood solution $s&#39;$ | More formally we can describe the selection of the best solution $s&#39;$ for iteration $k$ with the following set relation: . $s&#39; in N(s,k) = {N(s,k) - T(s,k) } + A(s,k)$ . Code . import math def generate_first_solution(): pass def generate_neighbourhood(): pass def objective_function(): pass def check_termination_criteria(): pass neighbourhood_solution = generate_first_solution() termination_criteria = False k = 0 # iteration cycle tabu_list = {neighbourhood_solution: 1} tabu_tenure = int(math.sqrt(100)) # CHANGE best_solution = neighbourhood_solution best_objective_val = math.inf while not termination_criteria: # update iteration k += 1 # initialize best neighbouring objective value and generate neighbours neighbourhood_objective_val = objective_function(neighbourhood_solution) neighbours = generate_neighbourhood(neighbourhood_solution) # iterate each neighbour for n in neighbours: current_objective_val = objective_function(n) # find the best neighbour objective value that is not tabu # we&#39;re assuming we want to minimize the objective value in this scenario if (n not in tabu_list) and (current_objective_val &lt; neighbourhood_objective_val): neighbourhood_objective_val = current_objective_val neighbourhood_solution = n # aspiration criteria (best solution obtained so far) if current_objective_val &lt; best_objective_val: best_objective_val = current_objective_val best_solution = n # update tabu list for k,v in list(tabu_list.items()): tabu_list[k] += 1 if tabu_list[k] == tabu_tenure: del tabu_list[k] # add best neighbourhood solution to tabu list tabu_list[current_solution] = 1 # check if termination criteria has been met yet if check_termination_criteria(): termination_criteria = True . Travelling Salesman Problem . The travelling salesman problem is a commonly studied optimization problem that tries to optimize the shortest possible route when visiting a list of $n$ cities. In the figure below, we have an example scenario where we&#39;re trying to find the shortest route. Each destination is denoted as point in the figure. . import matplotlib.pyplot as plt import random random.seed(42) num_cities = 12 coordinates = [(round(random.uniform(0, 10), 2), round(random.uniform(0, 10), 2)) for _ in range(num_cities)] coordinates = coordinates + [coordinates[0]] plt.scatter(*zip(*(coordinates))) plt.title(&#39;Coordinates of Cities&#39;) plt.xlabel(&#39;X Coordinate&#39;) plt.ylabel(&#39;Y Coordinate&#39;) plt.show() . Search Space . When visiting relatively few cities, it&#39;s tangible in finding an optimal solution. However, it becomes quite unfeasible the more cities you visit. For $n$ destinations, we can derive a total of $ frac{(n-1)!}{2}$ possible routes. . n Number of paths Time (1 μs/chemin) . 5 | $12$ | $12$ μs | . 10 | $181,440$ | $0.18$ s | . 15 | $4.359 times 10^{10}$ | $12$ h | . 20 | $6.082 times 10^{16}$ | $1,928$ years | . 61 | $4.160 times 10^{81}$ | $13.19 times 10^{67}$ years | . Possible Moves . In this example, there are a limited number of moves given each state $s$. Since we have a total of $n$ cities and only perform one swap at a time, we have ${n choose 2}$ (n choose 2) number of swaps, or: . ${n choose 2} = frac{n!}{(n-2)!2!}$ . For example, let&#39;s say we have 4 possible cities - the possible swaps would be the following combinations where we would be swapping the order of cities we visit. . [(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)] . Objective Function . In this example, we can set our objective function as the total Euclidean distance travelled. Other distance measures could also be used. . def find_distance(c1, c2): return ((c1[0] - c2[0])**2 + (c1[1] - c2[1])**2)**(1/2) def find_total_distance(coordinates): total_distance = 0 for i in range(1, len(coordinates)): total_distance += find_distance(coordinates[i-1], coordinates[i]) return total_distance . $ textrm{Distance} = sqrt{(x_{2}-x_{1})^{2} + (y_{2}-y_{1})^{2}}$ . def objective_fn(coordinates): return find_total_distance(coordinates) . Here we can see the route of a randomly generated path. It&#39;s clearly not optimized, let&#39;s see if our Tabu Search can solve it! . import matplotlib.pyplot as plt import random random.seed(42) num_cities = 12 coordinates = [(round(random.uniform(0, 10), 2), round(random.uniform(0, 10), 2)) for _ in range(num_cities)] coordinates = coordinates + [coordinates[0]] plt.scatter(*zip(*(coordinates))) plt.plot(*zip(*(coordinates))) plt.title(&#39;Coordinates of Cities&#39;) plt.xlabel(&#39;X Coordinate&#39;) plt.ylabel(&#39;Y Coordinate&#39;) plt.show() . Code . import ast import itertools import math import matplotlib.pyplot as plt import random def generate_first_solution(num_cities, seed=42): # initialize seed for coordinates to be consistent random.seed(seed) # generate 0,10 grid coordinates = [(round(random.uniform(0, 10), 2), round(random.uniform(0, 10), 2)) for _ in range(num_cities)] # append first destination since we want to end there coordinates = coordinates + [coordinates[0]] return coordinates def swap_coordinates(coordinates, swap): c1, c2 = swap coordinates[c1], coordinates[c2] = coordinates[c2], coordinates[c1] return coordinates def generate_neighbourhood(coordinates): indices = list(range(1,(len(coordinates)-1))) swaps = list(itertools.combinations(indices, 2)) neighbourhood = [swap_coordinates(coordinates, s).copy() for s in swaps] return neighbourhood def generate_random_solutions(coordinates, tabu_list): num_shuffled = math.factorial(len(coordinates))/(math.factorial(len(coordinates)-2)*2) shuffled = [] for _ in range((int(num_shuffled))): s = [coordinates[0]] + random.sample(coordinates[1:-1], len(coordinates[1:-1])) + [coordinates[-1]] if str(s) not in tabu_list: shuffled.append(s) return shuffled def find_distance(c1, c2): return ((c1[0] - c2[0])**2 + (c1[1] - c2[1])**2)**(1/2) def find_total_distance(coordinates): total_distance = 0 for i in range(1, len(coordinates)): total_distance += find_distance(coordinates[i-1], coordinates[i]) return total_distance def objective_function(coordinates): return find_total_distance(coordinates) def check_termination_criteria(total_iterations, k): if k &gt; total_iterations: return True return False def tabu_search(num_cities=4, total_iterations=1_000, display=True): initial_solution = generate_first_solution(num_cities) neighbourhood_solution = initial_solution termination_criteria = False k_iter = 0 # iteration cycle tabu_list = {str(neighbourhood_solution): 1} tabu_tenure = int(math.sqrt(total_iterations)) frequency_memory = {str(neighbourhood_solution): 1} best_solution = neighbourhood_solution best_objective_val = math.inf while not termination_criteria: # update iteration k_iter += 1 # restart diversification (fm value is arbitrary) if frequency_memory[str(neighbourhood_solution)] &gt; 10: neighbours = generate_random_solutions(initial_solution, tabu_list) neighbourhood_solution = neighbours[0] else: # initialize best neighbouring objective value and generate neighbours neighbourhood_objective_val = objective_function(neighbourhood_solution) neighbours = generate_neighbourhood(neighbourhood_solution) # iterate each neighbour for n in neighbours: current_objective_val = objective_function(n) # find the best neighbour objective value that is not tabu # we&#39;re assuming we want to minimize the objective value in this scenario if (str(n) not in tabu_list) and (current_objective_val &lt; neighbourhood_objective_val): neighbourhood_objective_val = current_objective_val neighbourhood_solution = n # aspiration criteria (best solution obtained so far) if current_objective_val &lt; best_objective_val: best_objective_val = current_objective_val best_solution = n # update tabu list for k,v in list(tabu_list.items()): tabu_list[k] += 1 if tabu_list[k] == tabu_tenure: del tabu_list[k] # add best neighbourhood solution to track frequency memory if str(neighbourhood_solution) in frequency_memory: frequency_memory[str(neighbourhood_solution)] += 1 else: frequency_memory[str(neighbourhood_solution)] = 1 # add best neighbourhood solution to tabu list tabu_list[str(neighbourhood_solution)] = 1 # check if termination criteria has been met yet if check_termination_criteria(total_iterations, k_iter): termination_criteria = True if display and k_iter % (total_iterations//10) == 0: print(f&quot;Iteration: {k_iter}&quot;) print(f&quot;Best Objective Value: {best_objective_val:.2f}&quot;) print(f&quot;Best Neighbourhood Objective Value: {neighbourhood_objective_val:.2f}&quot;) if display: print(initial_solution) plt.scatter(*zip(*(initial_solution))) plt.plot(*zip(*(initial_solution))) plt.title(&#39;Initial Solution&#39;) plt.xlabel(&#39;X Coordinate&#39;) plt.ylabel(&#39;Y Coordinate&#39;) plt.show() for k,v in frequency_memory.items(): if (v == 10) and (k != best_solution): restart_solution = ast.literal_eval(k) break print(&#39;Without Restart Diversification&#39;) print(restart_solution) plt.scatter(*zip(*(restart_solution))) plt.plot(*zip(*(restart_solution))) plt.title(&#39;Global Optima&#39;) plt.xlabel(&#39;X Coordinate&#39;) plt.ylabel(&#39;Y Coordinate&#39;) plt.show() print(&quot;Best Solution&quot;) print(best_solution) plt.scatter(*zip(*(best_solution))) plt.plot(*zip(*(best_solution))) plt.title(&#39;Best Solution&#39;) plt.xlabel(&#39;X Coordinate&#39;) plt.ylabel(&#39;Y Coordinate&#39;) plt.show() . tabu_search(num_cities=12, total_iterations=100, display=True) . Iteration: 10 Best Objective Value: 31.65 Best Neighbourhood Objective Value: 31.65 Iteration: 20 Best Objective Value: 31.65 Best Neighbourhood Objective Value: 31.65 Iteration: 30 Best Objective Value: 31.65 Best Neighbourhood Objective Value: 47.50 Iteration: 40 Best Objective Value: 31.65 Best Neighbourhood Objective Value: 33.48 Iteration: 50 Best Objective Value: 31.65 Best Neighbourhood Objective Value: 33.48 Iteration: 60 Best Objective Value: 31.65 Best Neighbourhood Objective Value: 39.79 Iteration: 70 Best Objective Value: 27.22 Best Neighbourhood Objective Value: 27.22 Iteration: 80 Best Objective Value: 27.22 Best Neighbourhood Objective Value: 27.22 Iteration: 90 Best Objective Value: 27.22 Best Neighbourhood Objective Value: 36.77 Iteration: 100 Best Objective Value: 27.22 Best Neighbourhood Objective Value: 32.15 [(6.39, 0.25), (3.4, 1.55), (8.06, 6.98), (8.09, 0.06), (2.2, 5.89), (6.5, 5.45), (0.27, 1.99), (2.19, 5.05), (4.22, 0.3), (8.92, 0.87), (7.36, 6.77), (2.75, 2.23), (6.39, 0.25)] . Without Restart Diversification [(6.39, 0.25), (8.09, 0.06), (8.92, 0.87), (6.5, 5.45), (8.06, 6.98), (7.36, 6.77), (2.2, 5.89), (0.27, 1.99), (2.19, 5.05), (2.75, 2.23), (3.4, 1.55), (4.22, 0.3), (6.39, 0.25)] . Best Solution [(6.39, 0.25), (8.09, 0.06), (8.92, 0.87), (8.06, 6.98), (7.36, 6.77), (6.5, 5.45), (2.2, 5.89), (2.19, 5.05), (0.27, 1.99), (2.75, 2.23), (3.4, 1.55), (4.22, 0.3), (6.39, 0.25)] . It looks like our solution works! In the second plot, we can see that our solution reaches a local optima and is stuck at that solution. This is a great example of why we need restart diversification and generate a new neighbourhood. After performing a restart, the solution seems to converge at a global optima and we&#39;re able to find a better optima! . References . [1] F. Glover, E. Taillard and E. Taillard, &quot;A user&#39;s guide to tabu search&quot;, Annals of Operations Research, vol. 41, no. 1, pp. 1-28, 1993. Available: 10.1007/bf02078647. . [2] F. Glover, &quot;Tabu Search: A Tutorial&quot;, Interfaces, vol. 20, no. 4, pp. 74-94, 1990. Available: 10.1287/inte.20.4.74. .",
            "url": "https://evandros-ks.github.io/fastblog/2022/01/13/Tabu_Search.html",
            "relUrl": "/2022/01/13/Tabu_Search.html",
            "date": " • Jan 13, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "SIR Model of Infectious Diseases",
            "content": "Background . Epidemiological models are a type of compartmental model describing disease dynamics on a population. In the case of infectious diseases, we model how the virus/disease interacts with each compartment of the system. Each compartment in this model is represented as a homogeneous system - or an individual within a population where each individual is equivalent. Different epidemiological models consider different compartments with the most common being the SIR model. The compartments in this model are categorized within the population as: . (S)usceptible | (I)nfected | (R)ecovered | . SIR Model . For our case, we&#39;ll be reviewing how to implement a discrete SIR model which describes the dynamics of the SIR compartments in discrete time intervals. This simple model allows only two possible state transitions: $ S rightarrow I $ and $I rightarrow R $. The rate of susceptible individuals becoming infected is described by ${ beta}$, the average number of contacts per individual per unit time and the recovery rate $ gamma$ described the rate for infected individuals to recover. The recovery rate $ gamma$ can also be thought as the inverse of the infectious period for an infected individual or inverse of the recovery period for an infected individual. Each state transition and scaling parameter is shown in the figure below. . Our model will also assume a closed system assuming no immigration, emigration, births, or deaths in the population. Therefore, the population, $N$, remains constant which gives us the following relation ($ ref{pop}$) for all the compartments at any time $t$. . $ N = S(t) + I(t) + R(t) label{pop} tag{1}$ . Given this relation ($ ref{pop}$), we must also consider how individuals switch states from the current time $t$ to the next day $t+1$. The number of recovered individuals ($ ref{rec}$) is simply dependent on the number of infected individuals that recover given the recovery rate $ gamma$. From this relation, we know the number of infected individuals is conditional on the outflow of recovered individuals and inflow of newly infected individuals ($ ref{inf}$). The outflow was previously described ($ ref{rec}$) based on the recovery rate while the inflow is scaled by the force of infection, $ beta I(t)$ representing the number of newly infected individuals from the susceptible population. Finally, the number of susceptible individuals can be described as the remaining individuals that have not been infected or recovered ($ ref{sus}$). . $ R(t+1) = R(t) + gamma I(t) label{rec} tag{2}$ $ I(t+1) = I(t) - gamma I(t) + beta I(t) S(t) label{inf} tag{3}$ $ S(t+1) = N - I(t+1) - R(t+1) label{sus} tag{4}$ . Code . import numpy as np import plotly.graph_objects as go import time from ipywidgets import interact, widgets, HTML . def simulate(beta: float = 0.001, infectious_period: int = 21, N: int = 100, days: int = 365): &quot;&quot;&quot;Simulates SIR model for an infectious disease. Args: beta: Transmission rate of disease per day. infectious_period: Number of days disease is transmissible for an infected individual. N: Size of population. days: Number of days to simulate disease spread not including t=0 (t=0 is when first patient is infected). Returns: S: Numpy array containing number of susceptible individuals for each discrete time point. I: Numpy array containing number of infected individuals for each discrete time point. R: Numpy array containing number of recovered individuals for each discrete time point. &quot;&quot;&quot; gamma = 1/infectious_period # Number of days to simulate (starting at t=0) days = days + 1 # Initial number of infected and recovered (t=0) I, R = np.zeros(days), np.zeros(days) I[0], R[0] = 1, 0 # Initial number of susceptible S = np.zeros(days) S[0] = N - I[0] - R[0] for t in range(days-1): R[t+1] = min(R[t] + gamma * I[t], N) I[t+1] = min(I[t] - gamma * I[t] + beta * I[t] * S[t], N) S[t+1] = max(N - I[t+1] - R[t+1], 0) return S, I, R, N def plot(S, I, R, N): fig = go.FigureWidget() fig.add_trace(go.Scatter(y=S/N, mode=&#39;lines&#39;, name=&#39;Susceptible&#39;)) fig.add_trace(go.Scatter(y=I/N, mode=&#39;lines&#39;, name=&#39;Infected&#39;)) fig.add_trace(go.Scatter(y=R/N, mode=&#39;lines&#39;, name=&#39;Recoverd&#39;)) fig.update_layout(title=&quot;SIR Model&quot;, title_x = 0.5, xaxis_title=&quot;Number of Days&quot;, yaxis_title=&quot;Percentage of Population (N)&quot;, ) return fig S, I, R, N = simulate() fig = plot(S, I, R, N) def update_params(**kwargs): S, I, R, N = simulate(**kwargs) print(kwargs) fig.data[0].y = S/N fig.data[1].y = I/N fig.data[2].y = R/N time.sleep(1/2) fig.show() interact(update_params, beta = widgets.FloatSlider(value=0.001, min=1/1_000, max=1/10, step=0.0001, readout_format=&#39;.4f&#39;), infectious_period = widgets.FloatSlider(value=21, min=1, max=100, step=1), N = widgets.IntSlider(value=100, min=0, max=10_000, step=1), days = widgets.IntSlider(value=365, min=0, max=365*2, step=1)) . &lt;function __main__.update_params(**kwargs)&gt; . Note: To view an interactive SIR plot and how each parameter ($ beta$, $ gamma$, $N$, and number of days $T$) affect the compartment dynamics click the Google Colab link at the top of the page. . Reference . Krickeberg, K., &amp; Pham, T. M. H. (2011). Epidemiology: Key to prevention. Springer Science &amp; Business Media. .",
            "url": "https://evandros-ks.github.io/fastblog/2021/12/21/SIR_Model.html",
            "relUrl": "/2021/12/21/SIR_Model.html",
            "date": " • Dec 21, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://evandros-ks.github.io/fastblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://evandros-ks.github.io/fastblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}